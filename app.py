import os
import re
from dataclasses import dataclass
from typing import List, Tuple

import streamlit as st
from pypdf import PdfReader
from openai import OpenAI

# =========================================================
# Default Sample PDF (repo ë‚´ í¬í•¨, ë§¤ìš° ì¤‘ìš”)
# =========================================================
DEFAULT_SAMPLE_PDF = "ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ì˜ ì§€ë°©ìƒìˆ˜ë„ ê´€ íŒŒì†ì‚¬ê³  ê°ì§€ ë° ëˆ„ìˆ˜ê´€ë¦¬ ì‹œìŠ¤í…œ ê°œë°œ.pdf"

# =========================================================
# App Config
# =========================================================
st.set_page_config(
    page_title="K-water ìˆ˜ë„ê´€ë¦¬ AI ë´‡ (ìš”ì•½ Â· ì˜ˆì¸¡ Â· ìš´ì˜ë³´ì¡°) 26.01.22 4pm",
    page_icon="ğŸ’§",
    layout="wide",
)

# --- Sidebar Hide ---
st.markdown(
    """
    <style>
        [data-testid="stSidebar"] { display: none; }
    </style>
    """,
    unsafe_allow_html=True,
)

# =========================================================
# OpenAI Client
# =========================================================
def init_openai() -> Tuple[OpenAI, str]:
    api_key = st.secrets.get("OPENAI_API_KEY")
    model = st.secrets.get("OPENAI_MODEL", "gpt-5.2")

    if not api_key:
        st.error("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤ (Streamlit Secrets í™•ì¸).")
        st.stop()

    os.environ["OPENAI_API_KEY"] = api_key
    return OpenAI(), model


client, DEFAULT_MODEL = init_openai()

# =========================================================
# System Prompt
# =========================================================
SYSTEM_PROMPT = """
ë‹¹ì‹ ì€ K-water ìƒí•˜ìˆ˜ë„ ë¶„ì•¼ë¥¼ ì§€ì›í•˜ëŠ” ìˆ˜ë„ê´€ë¦¬ AI ë´‡ì´ë‹¤.

[ì›ì¹™]
- ì˜ì‚¬ê²°ì • ë³´ì¡°ìì´ë©° ìµœì¢… ê²°ì •ìëŠ” ì¸ê°„
- ëª¨ë“  ì œì•ˆì€ ê·¼ê±°ì™€ ë¶ˆí™•ì‹¤ì„± ëª…ì‹œ
- ë‹¨ì •ì  í‘œí˜„ ê¸ˆì§€

[ì‘ë‹µ êµ¬ì¡°]
ê·¼ê±° â†’ í•´ì„ â†’ ì œì•ˆ â†’ ë¦¬ìŠ¤í¬ â†’ ì¶”ê°€ í™•ì¸ì‚¬í•­
"""

# =========================================================
# Utility Functions
# =========================================================
def normalize_text(text: str) -> str:
    text = text.replace("\x00", " ")
    text = re.sub(r"[ \t]+", " ", text)
    text = re.sub(r"\n{3,}", "\n\n", text)
    return text.strip()


def extract_pdf_text(file_obj) -> str:
    reader = PdfReader(file_obj)
    pages = []
    for i, page in enumerate(reader.pages):
        try:
            txt = page.extract_text() or ""
        except Exception:
            txt = ""
        if txt.strip():
            pages.append(f"[page {i+1}]\n{txt}")
    return normalize_text("\n\n".join(pages))


def chunk_text(text: str, size: int = 8000, overlap: int = 800) -> List[str]:
    chunks = []
    start = 0
    while start < len(text):
        end = min(len(text), start + size)
        chunks.append(text[start:end])
        if end == len(text):
            break
        start = max(0, end - overlap)
    return chunks


def call_llm(prompt: str, model: str) -> str:
    resp = client.responses.create(
        model=model,
        input=prompt,
        temperature=0.2,
    )
    return (resp.output_text or "").strip()

# =========================================================
# Summarization Logic
# =========================================================
@dataclass
class SummaryResult:
    merged: str
    key_points: str
    glossary: str


def summarize_report(text: str, model: str) -> SummaryResult:
    chunks = chunk_text(text)
    chunk_summaries = []

    for i, ch in enumerate(chunks, 1):
        prompt = f"""
ë‹¤ìŒì€ K-water ìƒí•˜ìˆ˜ë„ ë³´ê³ ì„œ ì¼ë¶€ì´ë‹¤ (chunk {i}/{len(chunks)}).
- ìˆ˜ì¹˜/ì§€í‘œ/ê³µì • ì¤‘ì‹¬ ìš”ì•½
- ì—°êµ¬ ë° ìš´ì˜ ê´€ì  í¬í•¨
- 10~12ì¤„ ì´ë‚´

[ì›ë¬¸]
{ch}
"""
        chunk_summaries.append(call_llm(prompt, model))

    merged = call_llm(
        f"""ë‹¤ìŒ ì²­í¬ ìš”ì•½ì„ í†µí•© ìš”ì•½í•˜ë¼ (800~1200ì).

{chr(10).join(chunk_summaries)}
""",
        model,
    )

    key_points = call_llm(
        f"""ë‹¤ìŒ ìš”ì•½ì„ ì‹¤ë¬´ììš© ë¸Œë¦¬í”„ë¡œ ì¬ì‘ì„±í•˜ë¼.

[ìš”ì•½]
{merged}
""",
        model,
    )

    glossary = call_llm(
        f"""ë‹¤ìŒ ìš”ì•½ì—ì„œ í•µì‹¬ ìš©ì–´ 20ê°œ ë‚´ì™¸ ìš©ì–´ì§‘ ì‘ì„±.

[ìš”ì•½]
{merged}
""",
        model,
    )

    return SummaryResult(merged, key_points, glossary)

# =========================================================
# UI
# =========================================================
st.title("ğŸ’§ K-water ìˆ˜ë„ê´€ë¦¬ AI ë´‡")

tab1, tab2 = st.tabs(["1ï¸âƒ£ ë³´ê³ ì„œ ìš”ì•½", "2ï¸âƒ£ ìˆ˜ë„ê´€ë¦¬ ë´‡ ì´ˆì•ˆ"])

# ---------------------------
# TAB 1: Summary
# ---------------------------
with tab1:
    st.subheader("ë³´ê³ ì„œ ì„ íƒ")

    use_sample = st.checkbox(
        "ğŸ“„ ìƒ˜í”Œ ë³´ê³ ì„œ ì‚¬ìš© (ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ ì§€ë°©ìƒìˆ˜ë„ ëˆ„ìˆ˜ê´€ë¦¬)",
        value=True,
    )

    uploaded = None
    sample_loaded = False

    if use_sample:
        if DEFAULT_SAMPLE_PDF and os.path.exists(DEFAULT_SAMPLE_PDF):
            uploaded = DEFAULT_SAMPLE_PDF
            sample_loaded = True
            st.success("ìƒ˜í”Œ PDFê°€ ìë™ ì„ íƒë˜ì—ˆìŠµë‹ˆë‹¤.")
        else:
            st.error(f"ìƒ˜í”Œ PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {DEFAULT_SAMPLE_PDF}")
    else:
        uploaded = st.file_uploader("PDF ì—…ë¡œë“œ", type=["pdf"])

    if uploaded and st.button("ìš”ì•½ ìƒì„±"):
        with st.spinner("ìš”ì•½ ìƒì„± ì¤‘..."):
            if sample_loaded:
                with open(uploaded, "rb") as f:
                    raw_text = extract_pdf_text(f)
            else:
                raw_text = extract_pdf_text(uploaded)

            st.session_state.summary = summarize_report(raw_text, DEFAULT_MODEL)

    if "summary" in st.session_state:
        s = st.session_state.summary
        st.subheader("í†µí•© ìš”ì•½")
        st.write(s.merged)
        st.subheader("ì‹¤ë¬´ ë¸Œë¦¬í”„")
        st.write(s.key_points)
        st.subheader("ìš©ì–´ì§‘")
        st.write(s.glossary)

# ---------------------------
# TAB 2: Bot Draft
# ---------------------------
with tab2:
    if "summary" not in st.session_state:
        st.warning("ë¨¼ì € ë³´ê³ ì„œë¥¼ ìš”ì•½í•˜ì„¸ìš”.")
    else:
        if st.button("ìˆ˜ë„ê´€ë¦¬ ë´‡ ì´ˆì•ˆ ìƒì„±"):
            with st.spinner("ì´ˆì•ˆ ìƒì„± ì¤‘..."):
                draft = call_llm(
                    f"""{SYSTEM_PROMPT}

ë‹¤ìŒ ë³´ê³ ì„œ ìš”ì•½ì„ ë°”íƒ•ìœ¼ë¡œ
K-water ìˆ˜ë„ê´€ë¦¬ AI ë´‡ ê¸°íš ì´ˆì•ˆì„ ì‘ì„±í•˜ë¼.

[ìš”ì•½]
{st.session_state.summary.merged}
""",
                    DEFAULT_MODEL,
                )
                st.session_state.bot_draft = draft

        if "bot_draft" in st.session_state:
            st.subheader("ìˆ˜ë„ê´€ë¦¬ ë´‡ ê¸°íš ì´ˆì•ˆ")
            st.write(st.session_state.bot_draft)
