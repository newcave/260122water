import os
import re
from dataclasses import dataclass
from typing import List, Tuple

import streamlit as st
from pypdf import PdfReader
from openai import OpenAI

# =========================================================
# App Config
# =========================================================
st.set_page_config(
    page_title="K-water ìˆ˜ë„ê´€ë¦¬ AI ë´‡ (ìš”ì•½ Â· ì˜ˆì¸¡ Â· ìš´ì˜ë³´ì¡°) 26.01.22 4pm",
    page_icon="ğŸ’§",
    layout="wide",
)

# =========================================================
# OpenAI Client
# =========================================================
def init_openai() -> Tuple[OpenAI, str]:
    api_key = st.secrets.get("OPENAI_API_KEY")
    model = st.secrets.get("OPENAI_MODEL", "gpt-5.2")

    if not api_key:
        st.error("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤ (Streamlit Secrets í™•ì¸).")
        st.stop()

    os.environ["OPENAI_API_KEY"] = api_key
    return OpenAI(), model


client, DEFAULT_MODEL = init_openai()

# =========================================================
# System Prompt (ìˆ˜ë„ê´€ë¦¬ ë´‡ í•µì‹¬)
# =========================================================
SYSTEM_PROMPT = """
ë‹¹ì‹ ì€ K-water ìƒí•˜ìˆ˜ë„ ë¶„ì•¼ë¥¼ ì§€ì›í•˜ëŠ” ìˆ˜ë„ê´€ë¦¬ AI ë´‡ì´ë‹¤.

[ëª©ì ]
1) ì—°êµ¬ìë¥¼ ìœ„í•œ ê³µì • AI í•˜ì´ë¸Œë¦¬ë“œ ì˜ˆì¸¡ ì§€ì›
2) í˜„ì¥ ìë£Œ ìˆ˜ì§‘ì„ í†µí•œ ëª¨í˜• ì„±ëŠ¥ í–¥ìƒ
3) ìˆ˜ë„ ìš´ì˜ ì˜ì‚¬ê²°ì •ì˜ ì•ˆì „í•œ ë³´ì¡°

[ì›ì¹™]
- ì˜ì‚¬ê²°ì • ë³´ì¡°ìì´ë©° ìµœì¢… ê²°ì •ìëŠ” ì¸ê°„ì´ë‹¤.
- ëª¨ë“  ìš´ì˜ ì¡°ì¹˜ëŠ” ê·¼ê±°ì™€ ë¶ˆí™•ì‹¤ì„±ì„ ëª…ì‹œí•œë‹¤.
- í˜„ì¥ ì¶”ê°€ ì—…ë¬´ë¥¼ ìµœì†Œí™”í•˜ëŠ” ë°©í–¥ì„ ìš°ì„ í•œë‹¤.
- ë‹¨ì •ì  í‘œí˜„ì„ ê¸ˆì§€í•œë‹¤.

[ì‘ë‹µ êµ¬ì¡°]
ê·¼ê±° â†’ í•´ì„ â†’ ì œì•ˆ â†’ ë¦¬ìŠ¤í¬ â†’ ì¶”ê°€ í™•ì¸ì‚¬í•­
"""

# =========================================================
# Tool Schemas (Bì•ˆ)
# =========================================================
TOOLS = [
    {
        "name": "query_document",
        "description": "K-water ë³´ê³ ì„œ/ìš´ì˜ê¸°ì¤€/ë§¤ë‰´ì–¼ ê²€ìƒ‰",
        "parameters": {
            "type": "object",
            "properties": {
                "query": {"type": "string"},
                "doc_type": {
                    "type": "string",
                    "enum": ["report", "manual", "regulation"]
                }
            },
            "required": ["query"]
        }
    },
    {
        "name": "analyze_timeseries",
        "description": "ìˆ˜ì§ˆÂ·ìˆ˜ìš”Â·ì—ë„ˆì§€ ì‹œê³„ì—´ ì˜ˆì¸¡",
        "parameters": {
            "type": "object",
            "properties": {
                "target_variable": {"type": "string"},
                "time_horizon": {"type": "string"},
                "model_type": {
                    "type": "string",
                    "enum": ["physical", "ml", "hybrid"]
                }
            },
            "required": ["target_variable", "time_horizon"]
        }
    },
    {
        "name": "diagnose_anomaly",
        "description": "ì´ìƒ ì›ì¸ í›„ë³´ ì§„ë‹¨",
        "parameters": {
            "type": "object",
            "properties": {
                "symptom": {"type": "string"},
                "location": {"type": "string"}
            },
            "required": ["symptom"]
        }
    },
    {
        "name": "recommend_action",
        "description": "SOP ê¸°ë°˜ ìš´ì˜ ì¡°ì¹˜ì•ˆ ì œì•ˆ(ë³´ì¡°)",
        "parameters": {
            "type": "object",
            "properties": {
                "issue": {"type": "string"},
                "urgency": {
                    "type": "string",
                    "enum": ["monitor", "check", "urgent"]
                },
                "human_approval_required": {"type": "boolean"}
            },
            "required": ["issue"]
        }
    },
    {
        "name": "collect_field_feedback",
        "description": "í˜„ì¥ ê´€ì°°Â·ì¡°ì¹˜ ê²°ê³¼ ìˆ˜ì§‘",
        "parameters": {
            "type": "object",
            "properties": {
                "observation": {"type": "string"},
                "action_taken": {"type": "string"},
                "outcome": {"type": "string"}
            },
            "required": ["observation"]
        }
    }
]

# =========================================================
# Utility Functions
# =========================================================
def normalize_text(text: str) -> str:
    text = text.replace("\x00", " ")
    text = re.sub(r"[ \t]+", " ", text)
    text = re.sub(r"\n{3,}", "\n\n", text)
    return text.strip()


def extract_pdf_text(uploaded_file) -> str:
    reader = PdfReader(uploaded_file)
    pages = []
    for i, page in enumerate(reader.pages):
        try:
            txt = page.extract_text() or ""
        except Exception:
            txt = ""
        if txt.strip():
            pages.append(f"[page {i+1}]\n{txt}")
    return normalize_text("\n\n".join(pages))


def chunk_text(text: str, size: int = 8000, overlap: int = 800) -> List[str]:
    chunks = []
    start = 0
    while start < len(text):
        end = min(len(text), start + size)
        chunks.append(text[start:end])
        if end == len(text):
            break
        start = max(0, end - overlap)
    return chunks


def call_llm(prompt: str, model: str) -> str:
    resp = client.responses.create(
        model=model,
        input=prompt,
        temperature=0.2,
    )
    return (resp.output_text or "").strip()


# =========================================================
# Summarization Logic
# =========================================================
@dataclass
class SummaryResult:
    merged: str
    key_points: str
    glossary: str


def summarize_report(text: str, model: str) -> SummaryResult:
    chunks = chunk_text(text)
    chunk_summaries = []

    for i, ch in enumerate(chunks, 1):
        prompt = f"""
ë‹¤ìŒì€ K-water ìƒí•˜ìˆ˜ë„ ë³´ê³ ì„œ ì¼ë¶€ì´ë‹¤ (chunk {i}/{len(chunks)}).
- ìˆ˜ì¹˜/ì§€í‘œ/ê³µì • ì¤‘ì‹¬ ìš”ì•½
- ì—°êµ¬ ë° ìš´ì˜ ê´€ì  í¬í•¨
- 10~12ì¤„ ì´ë‚´

[ì›ë¬¸]
{ch}
"""
        chunk_summaries.append(call_llm(prompt, model))

    merge_prompt = f"""
ë‹¤ìŒì€ ë³´ê³ ì„œ ì²­í¬ ìš”ì•½ì´ë‹¤.
ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ í†µí•© ìš”ì•½ì„ ì‘ì„±í•˜ë¼.

[ì¶œë ¥]
1) í†µí•© ìš”ì•½ (800~1200ì)
2) ì—°êµ¬Â·ìš´ì˜ ì‹œì‚¬ì  TOP 7
3) ë°ì´í„°/ë³€ìˆ˜ í›„ë³´ ëª©ë¡

[ì²­í¬ ìš”ì•½]
{chr(10).join(chunk_summaries)}
"""
    merged = call_llm(merge_prompt, model)

    key_prompt = f"""
ë‹¤ìŒ ìš”ì•½ì„ ê¸°ë°˜ìœ¼ë¡œ ì‹¤ë¬´ììš© ë¸Œë¦¬í”„ ì‘ì„±:

- í•œ í˜ì´ì§€ ìš”ì•½
- ì¦‰ì‹œ ì‹¤í—˜ ê°€ëŠ¥í•œ ì•„ì´ë””ì–´ 5ê°œ

[ìš”ì•½]
{merged}
"""
    key_points = call_llm(key_prompt, model)

    glossary_prompt = f"""
ë‹¤ìŒ ìš”ì•½ì—ì„œ í•µì‹¬ ìš©ì–´ 20ê°œ ë‚´ì™¸ ìš©ì–´ì§‘ ì‘ì„±:

[ìš”ì•½]
{merged}
"""
    glossary = call_llm(glossary_prompt, model)

    return SummaryResult(merged, key_points, glossary)


# =========================================================
# Bot Draft Generation
# =========================================================
def generate_bot_draft(summary: str, model: str) -> str:
    prompt = f"""
{SYSTEM_PROMPT}

ë‹¤ìŒ ë³´ê³ ì„œ ìš”ì•½ì„ ê·¼ê±°ë¡œ
"K-water ìˆ˜ë„ê´€ë¦¬ AI ë´‡" ê¸°íš ì´ˆì•ˆì„ ì‘ì„±í•˜ë¼.

[í•„ìˆ˜ í¬í•¨]
- ê³µì • AI í•˜ì´ë¸Œë¦¬ë“œ ì˜ˆì¸¡ êµ¬ì¡°
- í˜„ì¥ ìë£Œ ìˆ˜ì§‘ â†’ ì„±ëŠ¥ í–¥ìƒ ë£¨í”„
- ì‚¬ìš©ì ìœ í˜•ë³„ ê¸°ëŠ¥
- ì•ˆì „ì¥ì¹˜ ë° íœ´ë¨¼ ì¸ ë” ë£¨í”„
- 8ì£¼ êµ¬ì¶• ë¡œë“œë§µ

[ë³´ê³ ì„œ ìš”ì•½]
{summary}
"""
    return call_llm(prompt, model)


# =========================================================
# UI
# =========================================================
st.title("ğŸ’§ K-water ìˆ˜ë„ê´€ë¦¬ AI ë´‡")

with st.sidebar:
    st.header("ì„¤ì •")
    model = st.text_input("ëª¨ë¸", DEFAULT_MODEL)

tab1, tab2, tab3 = st.tabs(
    ["1ï¸âƒ£ ë³´ê³ ì„œ ìš”ì•½", "2ï¸âƒ£ ìˆ˜ë„ê´€ë¦¬ ë´‡ ì´ˆì•ˆ", "3ï¸âƒ£ ìˆ˜ë„ê´€ë¦¬ ì±—ë´‡"]
)

# Session State
if "summary" not in st.session_state:
    st.session_state.summary = None
if "bot_draft" not in st.session_state:
    st.session_state.bot_draft = ""
if "messages" not in st.session_state:
    st.session_state.messages = []

# ---------------------------
# TAB 1: Summary
# ---------------------------
with tab1:
    uploaded = st.file_uploader("K-water ìƒí•˜ìˆ˜ë„ PDF ì—…ë¡œë“œ", type=["pdf"])
    if uploaded and st.button("ìš”ì•½ ìƒì„±"):
        with st.spinner("ìš”ì•½ ìƒì„± ì¤‘..."):
            raw_text = extract_pdf_text(uploaded)
            st.session_state.summary = summarize_report(raw_text, model)

    if st.session_state.summary:
        s = st.session_state.summary
        st.subheader("í†µí•© ìš”ì•½")
        st.write(s.merged)
        st.subheader("ì‹¤ë¬´ ë¸Œë¦¬í”„")
        st.write(s.key_points)
        st.subheader("ìš©ì–´ì§‘")
        st.write(s.glossary)

# ---------------------------
# TAB 2: Bot Draft
# ---------------------------
with tab2:
    if not st.session_state.summary:
        st.warning("ë¨¼ì € ë³´ê³ ì„œë¥¼ ìš”ì•½í•˜ì„¸ìš”.")
    else:
        if st.button("ìˆ˜ë„ê´€ë¦¬ ë´‡ ì´ˆì•ˆ ìƒì„±"):
            with st.spinner("ì´ˆì•ˆ ìƒì„± ì¤‘..."):
                st.session_state.bot_draft = generate_bot_draft(
                    st.session_state.summary.merged, model
                )

        if st.session_state.bot_draft:
            st.subheader("ìˆ˜ë„ê´€ë¦¬ ë´‡ ê¸°íš ì´ˆì•ˆ")
            st.write(st.session_state.bot_draft)

# ---------------------------
# TAB 3: Chatbot (Mock Tool Mode)
# ---------------------------
with tab3:
    st.caption("âš ï¸ í˜„ì¬ëŠ” Tool í˜¸ì¶œì„ 'ì„¤ê³„ ìˆ˜ì¤€'ìœ¼ë¡œë§Œ ì‹œë®¬ë ˆì´ì…˜")

    for msg in st.session_state.messages:
        with st.chat_message(msg["role"]):
            st.write(msg["content"])

    user_input = st.chat_input("ìˆ˜ë„ ìš´ì˜ / ì˜ˆì¸¡ / ì´ìƒ ì§„ë‹¨ ì§ˆë¬¸ ì…ë ¥")

    if user_input:
        st.session_state.messages.append({"role": "user", "content": user_input})

        with st.spinner("ì‘ë‹µ ìƒì„± ì¤‘..."):
            prompt = f"""
{SYSTEM_PROMPT}

[ëŒ€í™” ë§¥ë½]
{st.session_state.messages}

[ì‚¬ìš©ì ì§ˆë¬¸]
{user_input}
"""
            answer = call_llm(prompt, model)

        st.session_state.messages.append(
            {"role": "assistant", "content": answer}
        )

        with st.chat_message("assistant"):
            st.write(answer)

